{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9472f7fb-8ed2-4be6-8a31-8adf5352aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a9d6487-f4bd-494c-a8d6-9eff069c8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from permutations import kendalltau_dist\n",
    "# from permutations import aggregate_parity_new, aggregate_kemeny_new\n",
    "# from permutations import gen_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd0110fc-680d-4b24-872a-9f94c792bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "animelist = pd.read_csv(\"anime_data/animelist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "355ab284-198c-4782-84ce-ec635e839ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## watching status 2 == completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e86e4d-5ca9-47fa-bdd6-8786277734aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = animelist[animelist.watching_status == 2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ebb9b1-c4b1-4fcf-b819-4dda8e858381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320781"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31723e4c-13c6-4826-b49f-028c7d6cf070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16905"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed.anime_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8450df9-2897-4ab5-99ac-84810f2a2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "animes = pd.read_csv(\"anime_data/anime.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7607a37f-fe0a-47ea-9715-83e97e782951",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_anime_ids = completed.groupby(\"anime_id\").size().sort_values(ascending=False).head(15)\n",
    "top_animes = animes[animes['MAL_ID'].isin(top_anime_ids.index)]\n",
    "top_animes = top_animes.set_index('MAL_ID')\n",
    "top_animes = top_animes.loc[top_anime_ids.index].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f02c33e-c74b-4820-bf0b-bc3081378b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_rating_all_top_anime(completed, n_top_anime=20):\n",
    "    \"\"\"\n",
    "    Find users who have rated all of the top N anime and return their ratings.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    completed : DataFrame\n",
    "        DataFrame containing anime ratings with user_id and anime_id columns\n",
    "    n_top_anime : int, default=20\n",
    "        Number of top anime to consider\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    final_subset : DataFrame\n",
    "        Subset of ratings from users who rated all top N anime\n",
    "    top_anime_info : DataFrame\n",
    "        Information about the top N anime\n",
    "    \"\"\"\n",
    "    # Get the top N anime_ids\n",
    "    top_anime_ids = completed.groupby(\"anime_id\").size().sort_values(ascending=False).head(n_top_anime).index.tolist()\n",
    "    \n",
    "    # Filter to only include ratings for these top anime\n",
    "    top_anime_ratings = completed[completed['anime_id'].isin(top_anime_ids)]\n",
    "    \n",
    "    # Count how many of the top anime each user has rated\n",
    "    user_rating_counts = top_anime_ratings.groupby('user_id')['anime_id'].nunique()\n",
    "    \n",
    "    # Find users who have rated all top N anime\n",
    "    complete_users = user_rating_counts[user_rating_counts == n_top_anime].index.tolist()\n",
    "    \n",
    "    # Get the final subset of ratings from users who rated all top N anime\n",
    "    final_subset = top_anime_ratings[top_anime_ratings['user_id'].isin(complete_users)]\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Number of users who rated all top {n_top_anime} anime: {len(complete_users)}\")\n",
    "    print(f\"Total number of ratings in the final subset: {len(final_subset)}\")\n",
    "    return final_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72e5255f-73c9-45bf-9eb2-b8bbfa6244b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d=16\n",
    "# top100 = completed.groupby(\"anime_id\").size().sort_values(ascending=False).head(100).index.tolist()\n",
    "# random_subset = random.sample(top100, d)\n",
    "# random_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2cdf8ae-7fd9-4fdc-aacc-2101f394f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top100 = completed.groupby(\"anime_id\").size().sort_values(ascending=False).head(25).index.tolist()\n",
    "# random_subset = random.sample(top100, d)\n",
    "# top_anime_ratings = completed[completed['anime_id'].isin(random_subset)]\n",
    "\n",
    "# # Count how many of the top anime each user has rated\n",
    "# user_rating_counts = top_anime_ratings.groupby('user_id')['anime_id'].nunique()\n",
    "\n",
    "# # Find users who have rated all top N anime\n",
    "# complete_users = user_rating_counts[user_rating_counts == d].index.tolist()\n",
    "\n",
    "# # Get the final subset of ratings from users who rated all top N anime\n",
    "# final_subset = top_anime_ratings[top_anime_ratings['user_id'].isin(complete_users)]\n",
    "\n",
    "# # Print some statistics\n",
    "# print(f\"Number of users who rated all top {d} anime: {len(complete_users)}\")\n",
    "# print(f\"Total number of ratings in the final subset: {len(final_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48955cf-1b78-4406-bdc8-5659a54a6cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8112c30f-9166-45b2-8066-4d5484be67ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users who rated all top 16 anime: 8703\n",
      "Total number of ratings in the final subset: 139248\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_subset = get_users_rating_all_top_anime(completed, n_top_anime=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef822e4d-93e3-42a5-ae28-045a8214528f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "9     33638\n",
       "8     31644\n",
       "10    29304\n",
       "7     17565\n",
       "0     12475\n",
       "6      7362\n",
       "5      3484\n",
       "4      1908\n",
       "3      1016\n",
       "2       466\n",
       "1       386\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_subset.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "756aee8a-231d-430c-bb78-08efe50244a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_subset.drop(columns=['watching_status', 'watched_episodes']).to_csv('anime_data/fairness_experiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b0a31-5a87-4d07-81a3-b79867773291",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c07be7ae-6c98-4260-811a-977eefb97f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139248, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df = pd.read_csv('anime_data/fairness_experiment.csv')\n",
    "rating_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ab701cd-4695-476b-b4d8-e7384148fb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8703"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df.user_id.nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "177c9705-5754-42f5-9253-ce856c98eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df_to_rankings(df):\n",
    "    \"\"\"\n",
    "    Transform a DataFrame with columns ['user_id', 'anime_id', 'rating']\n",
    "    to a list of lists, where each inner list contains anime_ids ranked by rating for a user (is a partial list).\n",
    "    Breaks ties arbitrarily by adding small random noise to ratings.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['user_id', 'anime_id', 'rating']\n",
    "        \n",
    "    Returns:\n",
    "        List of lists, where each inner list is a ranking of anime_ids for a user\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy['rating_with_noise'] = df_copy['rating'] + np.random.uniform(-0.5, 0.5, size=len(df_copy))\n",
    "    user_rankings = []\n",
    "    \n",
    "    for user_id, group in df_copy.groupby('user_id'):\n",
    "        # Sort by rating_with_noise in descending order (highest rating first)\n",
    "        sorted_group = group.sort_values('rating_with_noise', ascending=False)\n",
    "        anime_ranking = sorted_group['anime_id'].tolist()\n",
    "        user_rankings.append(anime_ranking)\n",
    "    \n",
    "    return user_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "350f5889-05c5-49cf-a3d8-f37cde05dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def create_uniform_coreset(all_rankings, coreset_size):\n",
    "    \"\"\"\n",
    "    Create a uniform random coreset of rankings by sampling with replacement.\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(all_rankings), size=coreset_size, replace=True)\n",
    "    uniform_coreset = [all_rankings[i] for i in indices]\n",
    "    return uniform_coreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "571008d9-eb12-474b-98fb-eb85ddb1d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def convert_rankings_to_indices(rankings, ground_set):\n",
    "#     element_to_index = {element: idx for idx, element in enumerate(ground_set)}\n",
    "#     index_to_element = {idx: element for idx, element in enumerate(ground_set)}\n",
    "#     indexed_rankings = []\n",
    "#     for ranking in rankings:\n",
    "#         indexed_ranking = [element_to_index[element] for element in ranking]\n",
    "#         indexed_rankings.append(indexed_ranking)\n",
    "#     return np.array(indexed_rankings), element_to_index, index_to_element\n",
    "\n",
    "# def convert_indices_to_rankings(indexed_rankings, index_to_element):\n",
    "#     \"\"\"\n",
    "#     Convert numpy array of indices back to original rankings.\n",
    "    \n",
    "#     Args:\n",
    "#         indexed_rankings: numpy.ndarray of indices\n",
    "#         index_to_element: dict mapping indices to original elements\n",
    "    \n",
    "#     Returns:\n",
    "#         list: List of rankings with indices replaced by original elements\n",
    "#     \"\"\"\n",
    "#     original_rankings = []\n",
    "#     for ranking in indexed_rankings:\n",
    "#         original_ranking = [index_to_element[idx] for idx in ranking]\n",
    "#         original_rankings.append(original_ranking)\n",
    "    \n",
    "#     return original_rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be9c09f-6cff-481e-a9c6-8a716373b18f",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfb017eb-c543-460e-ab25-65fac9efc6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fair_rank_aggregation import kemeny_rank_aggregation, kemeny_rank_aggregation_with_parity, compute_kendall_tau_cost, create_group_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25a40bf8-bf40-4280-a8d6-c0a40069cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = transform_df_to_rankings(rating_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec6b812c-da9b-413f-b8d1-5224679ae6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_set = set(rankings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acc57c20-3e38-4cf2-9a6e-a124c2e49f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fairness_experiment(ground_set, all_rankings, prob_values, delta=0.1, coreset_size=None, repetitions=20):\n",
    "    aggregated = kemeny_rank_aggregation(rankings, ground_set)\n",
    "    found = False\n",
    "    while not found:\n",
    "        groups_list = []\n",
    "        for p in prob_values:\n",
    "            group = create_group_assignment(aggregated, p)\n",
    "            if any(existing_group == group for existing_group in groups_list):\n",
    "                break\n",
    "            groups_list.append(group)\n",
    "        found = True\n",
    "    print(groups_list)\n",
    "    return _run_fairness_experiment(ground_set, all_rankings, groups_list, prob_values, delta, coreset_size, repetitions)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "782578f9-6676-4af5-be92-405ffc9d3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_fairness_experiment(ground_set, all_rankings, groups_list, prob_values, delta=0.1, coreset_sizes=None, repetitions=20):\n",
    "    \"\"\"\n",
    "    Run experiment to evaluate costs with fairness constraints across different coreset sizes.\n",
    "\n",
    "    Args:\n",
    "        n_candidates: Number of candidates in each ranking\n",
    "        all_rankings: The full dataset of rankings\n",
    "        groups_values: Group assignments for candidates corresponding to each p value\n",
    "        p_values: List of p values (constraint parameters) from 0.5 to 1\n",
    "        delta: Delta parameter for aggregate_parity_new\n",
    "        coreset_sizes: List of coreset sizes to evaluate\n",
    "\n",
    "    Returns:\n",
    "        results_dict: Dictionary containing experiment results\n",
    "    \"\"\"\n",
    "    if coreset_sizes is None:\n",
    "        coreset_sizes = np.arange(100, 1050, 50)\n",
    "\n",
    "    results = {\n",
    "        'coreset_sizes': coreset_sizes,\n",
    "        'p_values': prob_values,\n",
    "        'full_costs': {p: None for p in prob_values},\n",
    "        'optimal_rankings': {p: None for p in prob_values},\n",
    "        'costs': {p: [] for p in prob_values},\n",
    "        'stds': {p: [] for p in prob_values}\n",
    "    }\n",
    "\n",
    "    # Calculate optimal solutions and costs for full dataset\n",
    "    print(\"Computing baseline solutions using full dataset...\")\n",
    "    for i in tqdm(range(len(groups_list)), desc=\"Processing p values\"):\n",
    "        group = groups_list[i]\n",
    "        p = prob_values[i]\n",
    "        try:\n",
    "            optimal_ranking = kemeny_rank_aggregation_with_parity(all_rankings, ground_set, group, delta)\n",
    "            \n",
    "            if optimal_ranking is not None:\n",
    "                optimal_cost = compute_kendall_tau_cost(optimal_ranking, all_rankings)\n",
    "                results['full_costs'][p] = optimal_cost\n",
    "                results['optimal_rankings'][p] = optimal_ranking\n",
    "            else:\n",
    "                print(f\"Failed to find optimal solution for p={p:.2f}\")\n",
    "                results['full_costs'][p] = np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"Error with p={p}: {e}\")\n",
    "            results['full_costs'][p] = np.nan\n",
    "\n",
    "    # Calculate costs for coresets\n",
    "    for size in tqdm(coreset_sizes, desc=\"Testing coreset sizes\"):\n",
    "        costs = {p: [] for p in prob_values}\n",
    "        for _ in range(repetitions):\n",
    "        # Create coreset\n",
    "            coreset = create_uniform_coreset(all_rankings, size)\n",
    "            for i in range(len(prob_values)):\n",
    "                group = groups_list[i]\n",
    "                p = prob_values[i]\n",
    "                coreset_ranking= kemeny_rank_aggregation_with_parity(coreset, ground_set, group, delta)\n",
    "                coreset_cost = compute_kendall_tau_cost(coreset_ranking, all_rankings)\n",
    "                costs[p].append(coreset_cost)\n",
    "        for p in prob_values:\n",
    "            results['costs'][p].append(np.mean(costs[p]))\n",
    "            results['stds'][p].append(np.std(costs[p]))\n",
    "    return results\n",
    "\n",
    "def plot_fairness_results_old(results):\n",
    "    \"\"\"\n",
    "    Create a professional paper-quality plot with smooth lines showing costs for\n",
    "    different fairness constraints across coreset sizes.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    colors = ['#0173B2', '#DE8F05', '#029E73', '#D55E00', '#CC78BC', '#CA9161']\n",
    "\n",
    "    \n",
    "    # Plot coreset results with smooth lines\n",
    "    for i, p in enumerate(results['p_values']):\n",
    "        # Convert to numpy array for easier handling\n",
    "        costs_array = np.array(results['costs'][p])\n",
    "        print(\"array\", costs_array)\n",
    "        \n",
    "        # Plot with smooth lines\n",
    "        plt.plot(results['coreset_sizes'], costs_array, '-', \n",
    "                color=colors[i % len(colors)], \n",
    "                label=f\"p={p:.2f} (Coreset)\", \n",
    "                linewidth=2.5)\n",
    "    \n",
    "    # Add horizontal lines for full dataset results\n",
    "    for i, p in enumerate(results['p_values']):\n",
    "        if not np.isnan(results['full_costs'][p]):\n",
    "            plt.axhline(y=results['full_costs'][p], linestyle='--', \n",
    "                      color=colors[i % len(colors)], \n",
    "                      label=f\"p={p:.2f} (Full Dataset)\", \n",
    "                      linewidth=2)\n",
    "    \n",
    "    # Improved styling\n",
    "    plt.xlabel('Coreset Size', fontsize=14)\n",
    "    plt.ylabel('Kendall Tau Cost', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Use log scale for y-axis to better visualize differences\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Add nice legend\n",
    "    plt.legend(fontsize=12, frameon=True, framealpha=0.95, edgecolor='gray', fancybox=False)\n",
    "    \n",
    "    # Set x-axis ticks\n",
    "    plt.xticks(results['coreset_sizes'][::4], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Make the spines visible but subtle\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(0.5)\n",
    "        spine.set_color('gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fairness_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3dfa623-711c-4faa-8216-20a7e177511b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8703"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ac4f1-a715-46be-ba0e-988fd00ac44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_subset = final_subset[['user_id', 'anime_id', 'rating']]\n",
    "final_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04120c9-7415-49ab-9b88-3bddf5aa71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_subset.columns\n",
    "final_subset.watched_episodes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c01ae-14d0-42dc-9603-5e81a7c91c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_subset[final_subset.rating==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc9e50-9ec9-48ca-af0c-e2a1d12c6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe296a45-576b-47ab-8377-2c289bf10a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fairness_results(results, num_repeats=20, std_devs=True):\n",
    "    \"\"\"\n",
    "    Create a professional paper-quality plot showing relative error (%) for\n",
    "    different fairness constraints across coreset sizes with regression lines and data points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Dictionary containing 'coreset_sizes', 'costs', 'full_costs', 'p_values' and 'stds'\n",
    "    num_repeats : int\n",
    "        Number of repetitions used to compute standard error\n",
    "    std_devs : bool, default=True\n",
    "        Whether to plot standard deviation bands\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    colors = ['#0173B2', '#DE8F05', '#029E73', '#D55E00', '#CC78BC', '#CA9161']\n",
    "    \n",
    "    # Plot relative error with regression lines and data points\n",
    "    for i, p in enumerate(results['p_values']):\n",
    "        # Skip if no full cost or if it's zero (to avoid division by zero)\n",
    "        if p not in results['full_costs'] or results['full_costs'][p] == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get the full dataset cost for this p value\n",
    "        full_cost = results['full_costs'][p]\n",
    "        \n",
    "        # Convert costs to relative error percentage: ((coreset_cost - full_cost) / full_cost) * 100\n",
    "        costs_array = np.array(results['costs'][p])\n",
    "        rel_errors = ((costs_array - full_cost) / full_cost) * 100\n",
    "        \n",
    "        # Get standard deviation array and convert to standard error if needed\n",
    "        if std_devs and 'stds' in results and p in results['stds']:\n",
    "            stds_array = np.array(results['stds'][p])\n",
    "            std_errors = (stds_array / np.sqrt(num_repeats)) / full_cost * 100\n",
    "        \n",
    "        # Calculate linear regression line if we have at least 2 points\n",
    "        if len(results['coreset_sizes']) >= 2:\n",
    "            # Create X range for regression line (extend slightly beyond the data)\n",
    "            x_range = np.linspace(min(results['coreset_sizes']) * 0.95, \n",
    "                                 max(results['coreset_sizes']) * 1.05, 100)\n",
    "            \n",
    "            # Linear regression (1st degree polynomial)\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "                results['coreset_sizes'], rel_errors)\n",
    "            line = slope * x_range + intercept\n",
    "            \n",
    "            # Plot the regression line\n",
    "            plt.plot(x_range, line, '-', \n",
    "                    color=colors[i % len(colors)], \n",
    "                    label=f\"p={p}\", \n",
    "                    linewidth=2.5,\n",
    "                    zorder=10)\n",
    "                \n",
    "            # Add confidence interval as a shaded area if requested\n",
    "            if std_devs and 'stds' in results and p in results['stds']:\n",
    "                plt.fill_between(results['coreset_sizes'], \n",
    "                                rel_errors - std_errors,\n",
    "                                rel_errors + std_errors,\n",
    "                                color=colors[i % len(colors)], \n",
    "                                alpha=0.15,\n",
    "                                zorder=5)\n",
    "        else:\n",
    "            # Just plot a single point if only one data point\n",
    "            plt.plot(results['coreset_sizes'], rel_errors, '-', \n",
    "                    color=colors[i % len(colors)], \n",
    "                    label=f\"p={p:.2f}\", \n",
    "                    linewidth=2.5,\n",
    "                    zorder=10)\n",
    "            \n",
    "            # Add confidence interval if requested\n",
    "            if std_devs and 'stds' in results and p in results['stds']:\n",
    "                plt.fill_between(results['coreset_sizes'], \n",
    "                                rel_errors - std_errors,\n",
    "                                rel_errors + std_errors,\n",
    "                                color=colors[i % len(colors)], \n",
    "                                alpha=0.15,\n",
    "                                zorder=5)\n",
    "        \n",
    "        # Add markers for the actual data points\n",
    "        plt.scatter(results['coreset_sizes'], rel_errors, \n",
    "                   color=colors[i % len(colors)], \n",
    "                   s=50,  # Size of dots\n",
    "                   zorder=15,  # Make sure dots are on top\n",
    "                   edgecolor='white',  # White edge makes dots stand out\n",
    "                   linewidth=1)\n",
    "    \n",
    "    # Add a horizontal line at y=0 (representing no difference from full dataset)\n",
    "    plt.axhline(y=0, linestyle='-', color='black', linewidth=1, alpha=0.5, zorder=5)\n",
    "    \n",
    "    # Improved styling\n",
    "    plt.xlabel('Coreset Size', fontsize=14)\n",
    "    plt.ylabel('Relative Error (%)', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Determine number of columns for legend based on number of p_values\n",
    "    ncol = 2\n",
    "    if len(results['p_values']) > 6:\n",
    "        ncol = 3\n",
    "    \n",
    "    # Add a multi-column legend in the upper left corner\n",
    "    plt.legend(fontsize=11, frameon=True, framealpha=0.95,\n",
    "              edgecolor='gray', fancybox=False,\n",
    "              loc='lower right',  # Position in upper left\n",
    "              bbox_to_anchor=(1, 0),  # Fine-tune position\n",
    "              ncol=ncol)  # Multiple columns for more compact width\n",
    "    \n",
    "    # Set x-axis ticks\n",
    "    if len(results['coreset_sizes']) > 4:\n",
    "        plt.xticks(results['coreset_sizes'][::2], fontsize=12)\n",
    "    else:\n",
    "        plt.xticks(results['coreset_sizes'], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Make the spines visible but subtle\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(0.5)\n",
    "        spine.set_color('gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/fairness_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()  # Return the figure for further customization if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab587c0-c1b4-48fe-a0e8-efbf0dea2b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{5114: 0, 2904: 0, 32281: 0, 1575: 1, 1535: 1, 30276: 1, 199: 1, 16498: 1, 4224: 1, 19815: 0, 6547: 1, 31964: 0, 20: 1, 22319: 0, 10620: 0, 11757: 0}, {5114: 0, 2904: 1, 32281: 1, 1575: 0, 1535: 0, 30276: 0, 199: 0, 16498: 1, 4224: 0, 19815: 1, 6547: 0, 31964: 0, 20: 1, 22319: 1, 10620: 1, 11757: 1}, {5114: 0, 2904: 0, 32281: 0, 1575: 0, 1535: 0, 30276: 0, 199: 0, 16498: 0, 4224: 1, 19815: 1, 6547: 1, 31964: 1, 20: 1, 22319: 1, 10620: 1, 11757: 1}]\n",
      "Computing baseline solutions using full dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing p values: 100%|████████████████████████| 3/3 [00:07<00:00,  2.35s/it]\n",
      "Testing coreset sizes:  42%|████████▊            | 8/19 [08:42<12:04, 65.89s/it]"
     ]
    }
   ],
   "source": [
    "repetitions = 20\n",
    "start_time = time.time()\n",
    "results = run_fairness_experiment(ground_set, rankings, [0.5,0.25,0.125], delta=0.1, coreset_size=None, repetitions=repetitions)\n",
    "print(f\"took {time.time() - start_time} seconds\")\n",
    "plot_fairness_results(results, repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d495f6-a00c-4745-b061-79e78096d9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness_results(results, repetitions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
